# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

from ultralytics.engine.results import Results
from ultralytics.models.yolo.detect.predict import DetectionPredictor
from ultralytics.utils import DEFAULT_CFG, ops


class SegmentationPredictor(DetectionPredictor):
    """
    A class extending the DetectionPredictor class for prediction based on a segmentation model.

    This class specializes in processing segmentation model outputs, handling both bounding boxes and masks in the
    prediction results.

    ä¸€ä¸ªæ‰©å±•äº† DetectionPredictor ç±»çš„ç±»ï¼Œç”¨äºåŸºäºåˆ†å‰²æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚

    è¯¥ç±»ä¸“é—¨å¤„ç†åˆ†å‰²æ¨¡å‹çš„è¾“å‡ºï¼Œå¤„ç†é¢„æµ‹ç»“æœä¸­çš„è¾¹ç•Œæ¡†å’Œæ©ç ã€‚

    Attributes:
        args (Dict): Configuration arguments for the predictor. é¢„æµ‹å™¨çš„é…ç½®å‚æ•°ã€‚
        model (torch.nn.Module): The loaded YOLO segmentation model. åŠ è½½çš„ YOLO åˆ†å‰²æ¨¡å‹ã€‚
        batch (List): Current batch of images being processed. æ­£åœ¨å¤„ç†çš„å½“å‰å›¾åƒæ‰¹æ¬¡ã€‚

    Methods:
        postprocess: Applies non-max suppression and processes detections. åº”ç”¨éæœ€å¤§æŠ‘åˆ¶å¹¶å¤„ç†æ£€æµ‹ç»“æœã€‚
        construct_results: Constructs a list of result objects from predictions. ä»é¢„æµ‹ä¸­æ„é€ ç»“æœå¯¹è±¡åˆ—è¡¨ã€‚
        construct_result: Constructs a single result object from a prediction. ä»é¢„æµ‹ä¸­æ„é€ å•ä¸ªç»“æœå¯¹è±¡ã€‚

    Examples:
        >>> from ultralytics.utils import ASSETS
        >>> from ultralytics.models.yolo.segment import SegmentationPredictor
        >>> args = dict(model="yolo11n-seg.pt", source=ASSETS)
        >>> predictor = SegmentationPredictor(overrides=args)
        >>> predictor.predict_cli()
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """
        Initialize the SegmentationPredictor with configuration, overrides, and callbacks.
        ä½¿ç”¨é…ç½®ã€è¦†ç›–å’Œå›è°ƒåˆå§‹åŒ– SegmentationPredictorã€‚
        """
        super().__init__(cfg, overrides, _callbacks)
        self.args.task = "segment"

    def postprocess(self, preds, img, orig_imgs):
        """
        Apply non-max suppression and process detections for each image in the input batch.
        å¯¹è¾“å…¥æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªå›¾åƒåº”ç”¨éæå¤§å€¼æŠ‘åˆ¶å¹¶å¤„ç†æ£€æµ‹ç»“æœã€‚
        """
        # Extract protos - tuple if PyTorch model or array if exported
        # æå–åŸå‹ - å¦‚æœæ˜¯ PyTorch æ¨¡å‹åˆ™ä¸ºå…ƒç»„ï¼Œå¦‚æœæ˜¯å¯¼å‡ºçš„åˆ™ä¸ºæ•°ç»„
        protos = preds[1][-1] if isinstance(preds[1], tuple) else preds[1]
        return super().postprocess(preds[0], img, orig_imgs, protos=protos)

    def construct_results(self, preds, img, orig_imgs, protos):
        """
        Construct a list of result objects from the predictions.
        ä»é¢„æµ‹ä¸­æ„é€ ç»“æœå¯¹è±¡åˆ—è¡¨ã€‚

        Args:
            preds (List[torch.Tensor]): List of predicted bounding boxes, scores, and masks. é¢„æµ‹çš„è¾¹ç•Œæ¡†ã€åˆ†æ•°å’Œæ©ç åˆ—è¡¨ã€‚
            img (torch.Tensor): The image after preprocessing. é¢„å¤„ç†åçš„å›¾åƒã€‚
            orig_imgs (List[np.ndarray]): List of original images before preprocessing. é¢„å¤„ç†å‰çš„åŸå§‹å›¾åƒåˆ—è¡¨ã€‚
            protos (List[torch.Tensor]): List of prototype masks. åŸå‹æ©ç åˆ—è¡¨ã€‚

        Returns:
            (List[Results]): List of result objects containing the original images, image paths, class names,
                bounding boxes, and masks. åŒ…å«åŸå§‹å›¾åƒã€å›¾åƒè·¯å¾„ã€ç±»åã€è¾¹ç•Œæ¡†å’Œæ©ç çš„ç»“æœå¯¹è±¡åˆ—è¡¨ã€‚
        """
        return [
            self.construct_result(pred, img, orig_img, img_path, proto)
            for pred, orig_img, img_path, proto in zip(preds, orig_imgs, self.batch[0], protos)
        ]

    def construct_result(self, pred, img, orig_img, img_path, proto):
        """
        Construct a single result object from the prediction.
        ä»é¢„æµ‹ä¸­æ„é€ å•ä¸ªç»“æœå¯¹è±¡ã€‚

        Args:
            pred (np.ndarray): The predicted bounding boxes, scores, and masks. é¢„æµ‹çš„è¾¹ç•Œæ¡†ã€åˆ†æ•°å’Œæ©ç ã€‚
            img (torch.Tensor): The image after preprocessing. é¢„å¤„ç†åçš„å›¾åƒã€‚
            orig_img (np.ndarray): The original image before preprocessing. é¢„å¤„ç†å‰çš„åŸå§‹å›¾åƒã€‚
            img_path (str): The path to the original image. åŸå§‹å›¾åƒçš„è·¯å¾„ã€‚
            proto (torch.Tensor): The prototype masks. åŸå‹æ©ç ã€‚

        Returns:
            (Results): Result object containing the original image, image path, class names, bounding boxes, and masks.
                åŒ…å«åŸå§‹å›¾åƒã€å›¾åƒè·¯å¾„ã€ç±»åã€è¾¹ç•Œæ¡†å’Œæ©ç çš„ç»“æœå¯¹è±¡ã€‚
        """
        if not len(pred):  # save empty boxes - ä¿å­˜ç©ºæ¡†
            masks = None
        elif self.args.retina_masks:
            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)
            masks = ops.process_mask_native(proto, pred[:, 6:], pred[:, :4], orig_img.shape[:2])  # HWC
        else:
            masks = ops.process_mask(proto, pred[:, 6:], pred[:, :4], img.shape[2:], upsample=True)  # HWC
            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)
        if masks is not None:
            keep = masks.sum((-2, -1)) > 0  # only keep predictions with masks - åªä¿ç•™æœ‰æ©ç çš„é¢„æµ‹
            pred, masks = pred[keep], masks[keep]
        return Results(orig_img, path=img_path, names=self.model.names, boxes=pred[:, :6], masks=masks)
